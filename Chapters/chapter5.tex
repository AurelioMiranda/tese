%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter6.tex
%% NOVA thesis document file
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter6.tex}%

\chapter{Software Testing}
\label{cha:test}


\epigraph{ \textit{This chapter documents the software testing done throughout the development of the thesis. It highlights how the specific tests where done, in what context and their results.}}

Software testing was conducted specifically for the configuration of the code generation part of \gls{RAMSES}. This means that the goal was to ensure that the newly developed configuration options did not modify the runtime results of the generated software, hence maintaining the core goal of the input model.

\section{Integration Testing}
\label{sec:test_feature}

This type of testing is recurrent along the development, it serves to ensure that the features implemented are functioning as intended and that the previous version of the software has retained its functionality when new features were added.

Integration testing was performed in two clear stages, matching the incremental development of the configuration language. The first phase, in August, ensured that the addition of configuration features did not change the behavior of the baseline generator. The second phase, in October, focused on the validation of the more advanced architectural and runtime-related configuration options and refinement of the testing methodology to better reflect the intended modularity of \gls{RAMSES}.

\subsection{Integration Testing: August}
\label{sec:test_feature_one}

After the development of the first batch of features, the testing served to ensure that the previous version of the code generator was still working as intended and to validate the integration of the code generator configurator in the pipeline.

The main testing environment for the tests in this sub section was the following:

\begin{itemize} 
	\item Eclipse version 2024-03 (4.31)
	\item OSATE2 version 2.16.0.vfinal
	\item RAMSES2 version Aug 26, 2025 (commit 335579b6e58a188475e7a5e7ed099b9a549bfb25)
	\item ROS 2 Humble Hawksbill
	\item Acceleo version 3.7.15.202402190857
	\item MDELab Workflow version 1.9.1.202402292002
	\item EMF - Eclipse Modeling Framework SDK version 2.37.0.v20240203-1010
	\item Ubuntu version 22.04.5 LTS
\end{itemize}

The regression tests applied at this time were limited since the previous version of the software did not include any code generation customization, only the simple transition from model to code (\gls{M2T}), which, after generating a \gls{ROS} project in both the previous code generator and the at the time current one (with the first batch of features implemented), it was possible to verify that both generators still produced the same practical output, essentially, the code produced by both generators yields the same functionality.

The integration test part was done much more in depth since the goal was to intertwine the different new features and find out if they worked in perfect harmony as expected. The tests present in Table~\ref{tab:int_tests_1} were conducted.

\bgroup
\rowcolors{1}{}{GhostWhite}
\begin{xltabular}{\textwidth}{X X X X X}
	\caption{Feature dependency table}
	\label{tab:int_tests_1}\\
	\toprule
	\rowcolor{Gainsboro}%
	Test Case ID & Feature & Description & Expected Outcome \\
	\midrule
	TC-INT-01 & \Gls{identifier} Management & Check that \gls{identifier} config is applied in code, traceability, and report. & Same naming convention is visible in all outputs. \\
	TC-INT-02 & Comments & Verify comment style and formatting are preserved in all files. & All comments retain exact style and placement. \\
	TC-INT-03 & Traceability & Ensure model elements map to the correct code location and report entry. & JSON and HTML show accurate paths and line numbers. \\
	TC-INT-04 & Report Generation & Ensure generation metrics are passed from Java generator to report. & Report displays correct counts and settings. \\
	TC-INT-05 & Code Quality \& Report Sync & Verify quality checker warnings appear in both XML and report. & Both outputs show the same warning details. \\
	TC-INT-07 & Cross-Feature Stability & Test large models with custom \glspl{identifier}, comments, and style violations. & All features work together without breaking. \\
\end{xltabular}

After conducting these tests, the following was concluded:

\begin{itemize}
	\item \textbf{TC-INT-01 - \Gls{identifier} Consistency:} Verified that the \gls{identifier} configuration set in the \texttt{config.generator} model is applied consistently in generated C++ code and reflected in the traceability file as well as in the report.
	
	\item \textbf{TC-INT-02 - Comments:} Generated code containing both single-line and multi-line comments, verified that:
	\begin{enumerate}
		\item Acceleo templates correctly output the comments.
		\item Java generator preserves formatting.
	\end{enumerate}
	
	\item \textbf{TC-INT-03 - Traceability Link Accuracy:} Confirmed that model elements are mapped to the correct files and line numbers in generated code, and that these mappings are:
	\begin{enumerate}
		\item Present in the \gls{JSON} traceability output.
		\item Referenced in the \gls{HTML} report.
	\end{enumerate}
	
	\item \textbf{TC-INT-04 - Report Generation Data Flow:} Metrics collected during generation (number of components, code quality warnings) are passed to the Acceleo generator rendered correctly in the report.
	
	\item \textbf{TC-INT-05 - Code Quality \& Report Synchronization:} Triggered a known style violation in the model, generated code, and verified that:
	\begin{enumerate}
		\item The code quality checker detects it and outputs XML accurately referencing it.
		\item The Java generator correctly embeds this warning into the final HTML report.
	\end{enumerate}
	
	\item \textbf{TC-INT-07 - Cross-Feature Stability:} Used a large, complex model with:
	\begin{itemize}
		\item Custom \glspl{identifier}
		\item Extensive comments
		\item Known style violations
	\end{itemize}
	Verified that all features (\gls{identifier} config, comments, traceability, reporting, quality checking) work together without breaking any module.
\end{itemize}



\subsection{Integration Testing: October}
\label{sec:test_feature_two}

This latter stage of integration testing concentrated on the more architecture and runtime-oriented features of configuration, as well as revising the way in which testing was performed in order to better align with the modular design principles of \gls{RAMSES}. While tests in August concentrated on baseline correctness and compatibility, tests in October focused on verifying that these advanced configuration options (business logic decoupling, executor selection, file management, and build behavior) interoperate correctly and produce expected behavior when combined.

The testing environment remained mostly the same, with the exception of the execution platform: testing was moved from a physical Ubuntu installation on Windows to a dedicated Linux virtual machine. The \gls{RAMSES} version was also updated to Oct 27, 2025 (commit 3eb5118b57903cc5077ad4218fc83e182cbab199).

The initial tests were conducted with a "parameterized" way in mind, meaning that the architecture prioritized code efficiency albeit with an addition of complexity. This in of itself is not an issue at all, the tests were still very efficient, accurate and highly maintainable\footnote{To add a new test only one function needs to be added, at least.}. However, the current \gls{RAMSES} architecture benefited more from running tests in an isolated, standalone form, meaning that it would be better if it was possible to run specific test of the generated code without the need to run the whole test strip. For this goal, the parameterized way of testing was inefficient, since it entails that every test is generic and there is no way to quickly modify specific parameters in a single test.

So, the test class was re-written to have one method per test, per model. Meaning that instead of ten generic tests ran with parameterized values, there would be fifty \footnote{At that point in time, with five models.} specific tests, ten for each test model.

The features tested in Section~\ref{sec:test_feature_one} were re-tested and their behavior remained, as expected, the same. On top of that, new integration tests were performed to ensure that new features behave as intended, mainly:

\begin{itemize}
	\item \textbf{TC-INT-08 - Business Logic Decoupling:} Verified that when the DSL specifies decoupled business logic, the generated code separates logic into independent modules, allowing tasks to execute without unnecessary dependencies.
	
	\item \textbf{TC-INT-09 - Executor Behavior:} Configured models with different executor types and confirmed that:
	\begin{enumerate}
		\item Multi-threaded execution logic is correctly generated when requested.
		\item Static single-thread execution is applied as configured.
	\end{enumerate}
	
	\item \textbf{TC-INT-10 - File Management:} Generated code using models with various file settings and confirmed that:
	\begin{enumerate}
		\item Existing files remain unchanged when overwriting is disabled.
		\item Configured header prefixes are correctly applied in all files.
	\end{enumerate}
	
	\item \textbf{TC-INT-11 - Extension Script Execution:} Added pre and post generation scripts to the DSL and verified that:
	\begin{enumerate}
		\item Pre-generation scripts execute successfully before code generation starts.
		\item Post-generation scripts run correctly after generation completes.
	\end{enumerate}
	
	\item \textbf{TC-INT-12 - Build Integration Artifacts:} Configured build integration options in the DSL and confirmed that:
	\begin{enumerate}
		\item Source files, Makefile, package files, and launch scripts are generated according to the configuration.
		\item Disabling specific build outputs correctly prevents their generation without affecting other features.
	\end{enumerate}
	
	\item \textbf{TC-INT-13 - Legacy Library Handling:} Added legacy library configuration to the DSL and verified that:
	\begin{enumerate}
		\item Library headers are correctly included in the generated code.
		\item Libraries are copied into the project folder when requested.
	\end{enumerate}
	
	\item \textbf{TC-INT-14 - Target OS Adaptation:} Configured different target operating systems and confirmed that:
	\begin{enumerate}
		\item Generated code includes OS-specific adaptations for Linux, Windows, and MacOS.
	\end{enumerate}
\end{itemize}

These tests ensured that the new features provided, in fact, additional and specific functionalities that work as predicted.


\section{Regression Testing}
\label{sec:test_unit}

In order to automate the testing of the added features as development progressed unit tests were developed. By comparing the functional version of the generated projects against a newly generated version of the same project, we are able to detect differences in the end result which can point out flaws when a branch of the code generation is modified. 

The testing environment remained the same, with only the \gls{RAMSES} version changing throughout the development.

Essentially, what we test can be visually described by Figure~\ref{fig:testingFlowchart}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.55\textwidth]{testingFlowchart.png}
	\caption{Flowchart of the testing strategy for generated code (made by the author)}
	\label{fig:testingFlowchart}
\end{figure}

By using this architecture, it is only necessary to verify the correctness of the generated code once. The subsequent generations can be then compared to this already correct premise to determine errors or changes in the code that are not intended.

This verified control code was tested thoroughly by manually generating, compiling, deploying to a target system and executing a series of runtime tests to confirm its functionality.

An obvious flaw of this model is that, not only does it need complete manual validation in the start, but it's also very rigid when it comes to file comparison, comparing each file line-by-line\footnote{The comparison is not overly rigid and allows for exceptions such as the generated date and order changes in \gls{XML} files.}.

These flaws can also be seen as benefits, since the code needs to be tested rigorously to ensure maximum code quality when generated. This can only be achieve by actually executing the generated projects and verifying their functionality manually.

The unit tests were made to be as flexible as possible, allowing for different configurations and different models to be tested in a single run, providing insights in multiple generated projects in a single test execution as can be observed in Figure~\ref{fig:testingResults1}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.55\textwidth]{testingResults1.png}
	\caption{Testing results of 3 different models}
	\label{fig:testingResults1}
\end{figure}

The tests also account for certain projects not generating unneeded files such as the traceability or the report file. This behavior is translated by simply skipping the test as can be observed in Figure~\ref{fig:testingResultsSkip1}. Similar tests also verify the existence of folders and files that are a requirement for any \gls{ROS} project and need to be present after generation.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.55\textwidth]{testingResultsSkip1.png}
	\caption{Skipped Trace, CppCheck and Report files as those are not present}
	\label{fig:testingResultsSkip1}
\end{figure}

In addition, the test runs also validate the existence of executable both in the launch and make configurations, notably these executables have to exists in both files. 

As a guardrail, if files are present in the control project but not in the generated project that will generate an error and vice versa. This ensures that the generator produces only what is needed and no file is missing.

\bgroup
\rowcolors{1}{}{GhostWhite}
\begin{xltabular}{\textwidth}{X X X X X}
\caption{Regression test cases and expected outcomes}
\label{tab:reg_tests_1}\\
\toprule
\rowcolor{Gainsboro}%
Test Case ID & Feature & Description & Expected Outcome \\
\midrule
TC-REG-01 & Regression & Compare output with older version for unchanged models. & Output is functionally identical. \\
TC-REG-02 & Regression & Generate code for previously passing models. & Generation succeeds with no new errors. \\
TC-REG-03 & Regression & Measure generation time for unchanged models. & Timing does not exceed threshold. \\
TC-REG-04 & Regression & Test legacy models without new features enabled. & Output has no formatting or syntax changes. \\
\end{xltabular}


The tests described in table~\ref{tab:reg_tests_1} were proven true via \gls{JUnit} tests, which not only were able to verify the present tests but also additional ones that are not present in this table. The full detailed list of tests can be found in the Appendix~\ref{app:unit_tests}.

TC-REG-02, TC-REG-03 and TC-REG-04 were easily proven by the unit test strip since this process can be automated and viewed quickly. TC-REG-01 added a layer of complexity because it pushed for manual verification which had to be done to ensure the functionality of the output code remained the same as the control group.

All the unit tests previously described in this section (Section~\ref{sec:test_unit}) were initially done as plug-in tests, however, due to a multitude of reasons, they were later changed to standalone tests.

\begin{itemize}  
	\item \textbf{Plug-in unit tests} are tests done by launching a new Eclipse instance that loads all the resources necessary and executes the unit tests. This approach is simpler to setup but takes significantly more time to run than the standalone option due to the creation of the additional Eclipse instance.
	\item \textbf{Standalone unit tests} do not launch a new Eclipse instance, instead they run detached from eclipse, which makes them faster with the only downside being they require more initial setup to initialize the resources and dependencies that Eclipse would normally provide.
\end{itemize}

The change from plug-in tests to standalone revealed a clear advantage when it came to test execution time, out of 20 executions there was an average of 20\% of time saved, which can be attributed to the standalone runs not having to initialize a new instance of Eclipse. This time reduction is inversely proportional to the amount of tests, as in, less tests will translate to an even faster average of time saved by the standalone execution, which is especially useful when running single or low amounts of tests, having an average of 60\% of time saved. Even when running large amounts of tests (>100) there is still a reduced execution time on the standalone run, albeit not as noticeable as before with around 10\% reduction. The test run times can be seem in the Appendix~\ref{app:unit_test_times}.



\section{Usability Validation}
\label{sec:test_val}

With the \gls{DSL} finalized and its features implemented it was then possible to assess how an end user will interact with the newly developed features. The object of study is the developed \gls{DSL} and configuration workflow it uses. The main purpose of this usability validation test is to evaluate, understand, and improve the code configuration \gls{DSL} and workflow.

This testing will focus particularly on user efficiency (time to complete tasks) and effectiveness (task completion rate) of accomplish tasks, as well as how fast they learn the configuration itself, how often mistakes are made and how comfortable they feel using it. This information will not only be valuable to the current thesis but also to take into account when developing the complete \gls{RAMSES} tool with an integrated place for the configuration language. % TODO: confirm this

For these tests the participants consisted of end users with moderate \gls{AADL} knowledge and a highly experienced \gls{ROS} developer\footnote{7+ years of commercial \gls{ROS} usage.} that directly represents the target demographic.

The tests were conducted using the \gls{DSL} within the \gls{RAMSES} code generation workflow, operating in a Windows laptop with Osate with the latest versions available at the moment of testing (see more on Section~\ref{sec:test_usabl_eval_crit}), using it with functional \gls{AADL} models.

In \gls{GQM} terms, this can be translated to:

\begin{itemize}  
	\item \textbf{Object}: The developed DSL and its configuration workflow.
	\item \textbf{Purpose}: Evaluate, understand, and improve the workflow.
	\item \textbf{Quality focus}: Assess usability attributes including efficiency, effectiveness, learnability, error rate, and user satisfaction.
	\item \textbf{Perspective}: From the viewpoint of end users with moderate \gls{AADL} knowledge and a highly experienced \gls{ROS} developer. %TODO: confirm
	\item \textbf{Context}: Within the RAMSES code generation workflow, using functional AADL models.
\end{itemize}


For this test, 5 users with a moderate/high familiarization to \gls{AADL} were picked to perform a sequence of tasks that mimicked the usage of the \gls{DSL} to its near fullest extent.

With the goals now in mind, we can also associate research questions to better understand if the goals were actually met in the end, those questions are the following:

\begin{itemize}
	\item \textbf{RQ1 (Effectiveness):} Can users successfully complete the tasks using the \gls{DSL} without errors?
	\item \textbf{RQ2 (Efficiency):} How quickly can users complete the tasks using the \gls{DSL}?
	\item \textbf{RQ3 (Learnability):} How quickly do users become proficient with the \gls{DSL} when performing the sequence of tasks?
	\item \textbf{RQ4 (Error rate):} What types and frequency of errors do users make while using the \gls{DSL}?
	\item \textbf{RQ5 (Satisfaction):} How do users perceive the usability of the \gls{DSL} in terms of comfort, confidence, and overall satisfaction?
\end{itemize}

Each of the research questions serves to directly evaluate a defined usability attribute (effectiveness, efficiency, learnability, error rate, and satisfaction).

\subsection{Operational Definition of Usability}
\label{sec:test_op_def_usability}

According to \gls{ISO} 9241-11, usability is defined in terms of effectiveness, 
efficiency, and satisfaction within a specified context of use. 
Table~\ref{tab:usability_operationalization} summarizes how each attribute is 
operationalized and measured in this study.

\bgroup
\rowcolors{1}{}{GhostWhite}
\begin{xltabular}{\textwidth}{X X X X X}
\caption{Operational definition of usability attributes used in the study}
\label{tab:usability_operationalization}\\
\toprule
\rowcolor{Gainsboro}%
	Usability Attribute & ISO 9241-11 Concept & Measurement \\
	\midrule
	Effectiveness & Accuracy and completeness & Percentage of tasks successfully completed without external assistance. \\
	Efficiency & Resources used to achieve goals & 		Time to complete each task (in seconds). Measured per participant and aggregated across tasks. \\
	Learnability & Time and effort to reach proficiency & Improvement in task completion time and reduction in errors across the ordered sequence of tasks. \\
	Error rate & Frequency and type of user errors & 	Number and classification of errors observed during each task (syntax misuse, misunderstanding of concepts, navigation errors, etc). \\ 
	Satisfaction & User comfort and acceptance & 	Post-task questionnaires including \gls{SUS} and NASA-\gls{TLX} ratings (mental demand, effort, frustration, perceived performance). \\ 
\end{xltabular}


To ensure the validity of the usability evaluation, it is important to clarify the target population and the sampling strategy used. In the context of the \gls{RAMSES} tool and its integration with \gls{ROS} systems, the expected user population consists primarily of:

\begin{itemize}
	\item Engineers and researchers familiar with \gls{AADL} with moderate experience in architecture modeling but not necessarily experts in code-generation frameworks or \gls{ROS}.
	\item A robotics developer with practical experience in \gls{ROS} that represented a major demographic expected to interact with the generated code and configuration artifacts.
\end{itemize}

Given the availability constraints inherent in specialized domains such as \gls{AADL} and safety-critical modeling, the participant group for this study constitutes a convenience sample. According to Falessi et al. (ST7/ST8)~\cite{Falessi_Juristo_Wohlin_Turhan_Münch_Jedlitschka_Oivo_2017}, convenience sampling is acceptable in empirical software engineering studies when the objective is exploratory evaluation, early-stage usability validation, and identification of practical issues affecting real users. While this sampling method limits statistical generalizability, it provides valuable insight into typical user behaviour within the actual intended user demographic.

To make this verification operational, each usability requirement introduced in Chapter~\ref{chap:challenges_and_requirements} is mapped to a specific dependent variable in the evaluation design. The requirement that the \gls{DSL} be human-readable and accessible to users with different technical backgrounds is assessed through learnability and effectiveness metrics, namely the time needed to complete configuration tasks and the rate of successfully completed steps. The requirement for low cognitive overhead and clarity of configuration rules is evaluated using the NASA-\gls{TLX} workload scale, which measures perceived mental demand, effort and frustration. The requirement that the \gls{DSL} support correct and error tolerant configuration is linked to the error rate and the qualitative analysis of error types. Similarly, the requirement for a declarative and predictable configuration is associated with efficiency metrics (task duration, need for backtracking) and with participant reports captured through the \gls{SUS} questionnaire. By associating each requirement with at least one measurable variable, the evaluation ensures that high level design goals such as readability, configurability, robustness, and accessibility are reflected in user experience and performance.


\subsection{Participants}
\label{sec:test_usabl_participants}

A total of 5 participants were selected for this study. All were familiar with \gls{AADL} and had experience with model-driven workflows, even though their backgrounds and technical depth varied. This group reflects the typical user base within the RAMSES ecosystem: developers, testers, and stakeholders who understand AADL semantics but may not necessarily be expert programmers. Participants remained anonymous.

The number of participants was not random, many studies have shown that there are conflicting opinions regarding the perfect amount of users needed to verify the usability of an application. Carol Barnum~\cite{Barnum_2003} strongly concludes that Nielsen's~\cite{Nielsen_2024} 5 user minimum is not adequate for many full fledged applications and system diversity, user diversity, test goals and risk tolerance can help decide a more appropriate amount. That same principal applies here but in a different manner because we are not simply testing a complete application but only a specific integrated tool that is part of the broader system. It is correct to say that more users will yield better overall results, however, since this tool is aimed at a very specific user base, each test must be conducted without failure. 
% TODO: im out of inspiration and this paragraph needs to be very concise, resume later

Out of all participants, only one of them interacted with a very early and incomplete version of the \gls{DSL} and, during the test, this participant did not entice any familiarization with the current \gls{DSL}, hence its safe to say that no participant understood what the \gls{DSL} was, what purpose it served and how to operate it before the execution of the test.

\subsubsection{Participant Characterization}
\label{sec:test_usabl_participant_character}

Table~\ref{tab:participant_character} summarizes the technical background of each participant.

\bgroup
\rowcolors{1}{}{GhostWhite}
\begin{xltabular}{\textwidth}{X X X X X}
	\caption{Operational definition of usability attributes used in the study}
	\label{tab:participant_character}\\
	\toprule
	\rowcolor{Gainsboro}%
	Participant & \gls{AADL} Experience & \gls{ROS} Experience & \gls{DSL} Exposure & Notes \\
	\midrule
	P1 & 2 year & Light (Work Research) & None & Primary background in model-driven engineering. \\
	P2 & 6 months & Light (Work Research) & Brief (Early version) & Accompanied the concept of the \gls{DSL} during development. \\
	P3 & 6 months & Light (Academic) & None & \gls{RAMSES} developer with no connection to the code generation module. \\
	P4 & 3 months & Medium (Academic) & None & Java developer with previous experience in \gls{AADL} and \gls{ROS}. \\
	P5 & 4 months & Light (Academic) & None & Frontend industry developer with academic experience in \gls{ROS} and C++. \\
	P5 & none & High (7+ Years Commercial) & None & Represents the highly experienced \gls{ROS} developer demographic. \\
\end{xltabular}

\textbf{Average \gls{AADL} Experience (P1-P5)}: $\approx 3.5$ years.


\textbf{Target Demographic Representation}: The sample includes five participants with moderate \gls{AADL} experience and one highly experienced \gls{ROS} professional (the expert user and generated code consumer).


\subsubsection{Inclusion and Exclusion Criteria}
\label{sec:test_usabl_participant_incl_excl}

\textbf{Inclusion Criteria}: Participants must have at least moderate undestanding of the \gls{AADL} language and model-driven engineering concepts.

\textbf{Exclusion Criteria}: Participants were excluded if they were directly involved in the design or implementation of the current \gls{DSL} version, with the exception of the one participant who had brief, early exposure, which was judged to not provide a significant advantage in the final version of the test.

\subsubsection{Recruitment and Blinding}
\label{sec:test_usabl_participant_rec_blind}

The participants were recruited via direct contact from professional networks associated with \gls{RAMSES} and safety-critical systems research, using the convenience sampling strategy.

Out of all participants, only P2 had brief interactions with a very early and incomplete version of the \gls{DSL} that excluded the \gls{UI} itself. During the test, this participant did not demonstrate any prior knowledge of the current \gls{DSL} features, supporting the assumption that all participants approached the tasks essentially as beginners of the final system.

The participants were not blinded to the end goal, which was presented as a "usability validation of a new code configuration workflow for \gls{RAMSES}." However, they were provided no task-specific guidance and were told the purpose of the test was to evaluate the tool, not the user. This minimized anxiety and the potential for a strong Hawthorne effect\footnote{When people behave differently because they know they are being watched}.


\subsection{Experimental Materials}
\label{sec:test_usabl_materials}

To ensure the test is valid and able to be reproduced, the technical environment and all its artifacts were the same across all testing sessions. The objective was to create a representative and consistent workflow environment for \gls{RAMSES} users.

\subsection{Hardware and Software Environment}
\label{sec:test_usabl_materials_env}

The tests were conducted under the following configuration:

\textbf{Operating System}: Microsoft Windows 10 (64-bit).

\textbf{Hardware}: A standardized Windows laptop running an Intel Core i5 processor with 8 GB of RAM was used for all sessions.

\textbf{\gls{AADL} Editor}: Osate (Open-Source Architecture Tool Environment) version 2.16.0.vfinal, based on Eclipse 2024-03 (4.31). This serves as the primary development environment for \gls{AADL} model authoring and \gls{RAMSES} plugin execution.

\textbf{Code Generation Tool}: The \gls{RAMSES} Code Generator, specifically the commit version 4de627aafdc2ea7c4940848b0dea7e293e382222, was utilized as an Osate plug-in. This component is responsible for processing the \gls{AADL} model alongside the \gls{DSL} configuration file to produce the target code.


In order to conduct the tests, the \gls{AAXL2} model \textit{Sys\_Rvision\_Sys\_Rvision\_Pi4\_Instance.aaxl2} was provided as input along with pre and post generated dummy scripts, with the main output being the generated \gls{RAMSES} project, ready to compile and execute.


\subsection{Tasks}
\label{sec:test_usabl_tasks}

Each participant was asked to perform a sequence of tasks designed to cover every major feature of the configuration language. These tasks simulate realistic use of the \gls{DSL} and its integration with \gls{RAMSES}. The task set was organized into five groups:

\begin{itemize}
	\item \textbf{Basic Configuration:} selecting the target language, target operating system, and output folder.
	\item \textbf{Identifier and Naming Options:} changing identifier style, applying prefixes or suffixes, and switching to the \textit{STANDARD} naming convention.
	\item \textbf{Feature Toggles:} enabling or disabling comments, report generation, traceability, and file management options.
	\item \textbf{Advanced Capabilities:} configuring executor type, managing business logic decoupling, referencing legacy libraries, and defining pre and post generation hooks.
	\item \textbf{Full Workflow Validation:} triggering code generation and verifying that the outputs (generated code, reports, trace files, etc) reflect the selected configuration.
\end{itemize}

Although it was not possible to test the entirety of the \gls{DSL} due to time, fatigue and stress constraints, the few options from every category means that it was possible to simulate a scenario close to real/industrial. Options such as selected the target language were prioritized over other options such as the copyright notice in the code comments section in order to maximize efficiency over time. The detailed task booklet can be found in the Appendix~\ref{app:task_book}.

The task sequence was intentionally designed to exercise the \gls{DSL} to near full extent without requiring prior explanation of its internal implementation with a low to medium difficulty level with each task. Moreover, the middle sections of the task booklet (sections B, C, D and E) were shuffled between candidates in order to mitigate the learning effects of a fixed order.


\subsection{Evaluation Criteria}
\label{sec:test_usabl_eval_crit}

To assess the usability attributes defined in Section~\ref{sec:test_op_def_usability}, three types of data were collected during the experimental sessions: quantitative metrics, qualitative observations, and subjective scoring.
% TODO: Clearly define all dependent variables and operationalization.

The usability evaluation followed the methodology defined in the requirements phase of the project. Both quantitative and qualitative measures were collected to assess the usability of the configuration interface and the effort required to use it.


\paragraph{Quantitative data}
\label{sec:test_usabl_quant}

The planned and methodical testing process along with the pilot test helped mitigate invalid runs entirely, this was very positive since it allowed for a better analysis of results and to the non-existence of discarding rules.
Task time data was gathered using a stop watch, where each task essentially corresponded to a "lap" on the stop watch, then, task times were associated by order. This method, although highly accurate, required full attention to detail since if a task was skipped, the timing would be off on the rest of the tasks, nevertheless, it was somewhat practical since error, misunderstandings and help requests still had to be answered at the same time. Thanks to the pilot test and the screen recording of each test, the negative effects of the timing method were greatly reduced and the data was gathered cleanly.
With the data now gathered, the processing consisted of averaging the time taken per task instead of the median since the time taken per task was relatively similar throughout the overall test, meaning that some tasks inadvertently took more time to be completed than others consistently throughout all the test.

For every task and participant, the following metrics were recorded:

\begin{itemize}
	\item Task completion time.
	\item Number and type of errors committed.
	\item Number of help requests or clarifications needed.
\end{itemize}



\paragraph{Qualitative data}
\label{sec:test_usabl_qual}

% TODO: Describe coding methodology for qualitative observations.
% TODO: Describe categories used for thematic analysis (if any predefined).

During the session, the following observations were captured:

\begin{itemize}
	\item Visible misunderstandings and their severity.
	\item User comments and verbalized expectations.
	\item Indicators of the learning curve across tasks (increased speed or confidence).
\end{itemize}


\paragraph{Subjective evaluation}
\label{sec:test_usabl_subj}

% TODO: Define scoring method for SUS (conversion, threshold interpretation).
% TODO: Define scoring method for NASA-TLX (weighted or unweighted?).

After completing all tasks, each participant completed two standardized questionnaires:

\begin{itemize}
	\item \textbf{\gls{SUS}} for perceived usability.
	\item \textbf{NASA \gls{TLX}} to assess perceived workload and cognitive demands.
\end{itemize}

These instruments provide widely accepted measures for usability and mental effort. They also enable a structured and comparable assessment.


\subsection{Execution}
\label{sec:test_usabl_proc}

% TODO: Add pilot test description, if any.
% TODO: Add moderator script summary or link to appendix.
% TODO: Describe measures taken to ensure consistent handling between sessions.
% TODO: Note any deviations from the predefined procedure.

Each session followed the same procedure:

\begin{enumerate}
	\item \textbf{Introduction:} participants were informed that they would configure a code generator using the provided interface, without receiving task-specific guidance.
	\item \textbf{Task execution:} participants completed the predefined task sequence while the evaluator recorded times, errors, and observations. No assistance was offered unless explicitly requested.
	\item \textbf{Post-study questionnaires:} participants completed the \gls{SUS} and NASA-\gls{TLX} questionnaires.
	\item \textbf{Final debriefing:} participants were invited to provide additional comments and suggestions.
\end{enumerate}

All sessions were conducted individually and under identical software environments to ensure comparability.


\subsection{Analysis}
\label{sec:test_usabl_analysis}

% TODO: Describe analysis procedure (statistical tests, or explain why none used).
% TODO: Add descriptive statistics: means, medians, SDs, CIs.
% TODO: Add interpretation rules (thresholds for acceptable performance).
% TODO: Clarify whether unit of analysis is per-task or per-user.

With all data collected, we can proceed with the analysis.

\begin{table}[htbp]
	\centering
	\caption{Usability Task Performance Summary}
	\label{tab:usability_tasks}
	\begin{tabular}{lcc}
		\toprule
		\textbf{Task} & \textbf{Average Completion Time (s)} & \textbf{Help Requests} \\
		\midrule
		Select target language &  & 2 \\
		Configure naming style &  & - \\
		Apply identifier prefix/suffix & & 2 \\
		Enable/disable comments &  & 1 \\
		Enable/disable report generation &  & 1 \\
		Enable/disable traceability &  & - \\
		Toggle business logic decoupling &  & - \\
		Change executor type &  & 2 \\
		Configure legacy libraries &  & 1 \\
		Enable file overwriting &  & - \\
		Set file prefix &  & - \\
		Manage build file generation &  & 3 \\
		Add pre/post scripts &  & 4 \\
		Trigger code generation &  & - \\
		Verify outputs &  & - \\
		\midrule
		\textbf{Totals} &  & 16 \\
		\bottomrule
	\end{tabular}
\end{table}

% TODO: Fill all missing data in table.
% TODO: Explain any spikes or anomalies with data-backed reasoning.

A clear spike on the help requests is the generation hooks/scripts and the managing of build file generation. These are mostly navigational help requests, were some users did not understand what the task was referring to given the specific name of the Build Integration\footnote{Which is the general way of mentioning the Make, Package, Source and Launch files.} and the \textit{Extension} hooks. As a consequence, the \textit{Build Integration} was simplified to \textit{Build} and the \textit{Extension} options were merged into the \textit{Files} options.

Table~\ref{tab:usability_tasks} illustrates the aggregated performance summary of the participants. The completion time is very much acceptable for most tasks given the lack of familiarity with the software that every participant had. We can see a spike when it comes to \gls{ROS} specific things that the casual \gls{AADL} user might not be familiar with, for instance, changing the thread executor, managing the business logic, etc.

% TODO: Support claims with quantitative evidence from table.
% TODO: Clarify what "acceptable" means (benchmark or requirement reference).

The amount of errors performed was below expectations, most likely due to the simplicity and structure of the configuration language as well as the straightforward task booklet.

These errors were quantified in the following Table~\ref{tab:usability_errors}.

\begin{table}[htbp]
	\centering
	\caption{Categorization of User Errors}
	\label{tab:usability_errors}
	\begin{tabular}{lcl}
		\toprule
		\textbf{Error Category} & \textbf{Count} & \textbf{Description} \\
		\midrule
		Navigation error & 8 & Incorrectly locating the configuration page or submenu. \\
		Misinterpretation & 5 & Misunderstanding the meaning of an option. \\
		UI confusion & 11 & Confusion caused by labels, layouts, or grouping. \\
		Missing confirmation & 4 & Forgetting to apply/save configuration changes. \\
		\bottomrule
	\end{tabular}
\end{table}



% TODO: Add explanation of how errors were categorized (predetermined taxonomy or post-hoc?).


\subsection{NASA \gls{TLX}}
\label{sec:test_usabl_nasa}

The NASA Task Load Index (NASA-\gls{TLX}) was used to assess perceived workload across six dimensions: mental demand, physical demand, temporal demand, perceived performance, effort, and frustration. Each dimension is scored on a scale from 0 (very low) to 100 (very high).

\textbf{Dimensions:}
\begin{itemize}
	\item Mental Demand
	\item Physical Demand
	\item Temporal Demand
	\item Performance
	\item Effort
	\item Frustration
\end{itemize}

\begin{table}[htbp]
	\centering
	\caption{NASA-\gls{TLX} Scores per Participant}
	\label{tab:nasatlx}
	\begin{tabular}{lcccccc}
		\toprule
		Participant & Mental & Physical & Temporal & Performance & Effort & Frustration \\
		\midrule
		P1 & 11 & 1 & 1 & 93 & 7 & 1 \\
		P2 & 27 & 4 & 4 & 87 & 22 & 5 \\
		P3 & 28 & 5 & 17 & 90 & 54 & 1 \\
		P4 & 9 & 17 & 3 & 100 & 18 & 1 \\
		P5 & 62 & 1 & 1 & 100 & 59 & 28 \\
		\bottomrule
	\end{tabular}
\end{table}

As can be observed in Table~\ref{tab:nasatlx}, most users have the same patterns, with only a few outliers. P5 experienced a higher mental demand than average, this can be due to the association of mental demand and focus, nevertheless, this ends up tying with the high performance score, in short, this user was focused on delivering the best. The temporal column reflects what was observed relatively well since nearly all users took less than 10 minutes to finish all tasks in a previously unknown software piece. Effort loosely correlated with mental demands, furthermore, the scores reflect well the actual tests, for instance, P1 was very relaxed and familiar with \gls{AADL} and \gls{ROS} so he understood and executed the tasks effortlessly as opposed to P3 and P5, which both had the lowest times working with \gls{ROS}. Lastly, frustration was unanimously low, which cemented the good, failure ready architecture that not only prevents the user from making mistakes but also pushes him in the right direction from the start.

Overall, the NASA-\gls{TLX} results indicate low to moderate cognitive workload, high perceived performance, and negligible frustration, which collectively point to good system usability and efficient cognitive support.

% TODO: Relate scores to system usability and cognitive effort.

\subsection{\gls{SUS}}
\label{sec:test_usabl_sus}

The \gls{SUS} was completed alongside the NASA \gls{TLX} at the end of the test to capture the usability of the \gls{DSL}. The SUS consists of 10 items scored on a 5-point Likert scale that can be found in the Appendix~\ref{app:sus} and the complete scores on Appendix~\ref{app:sus_scores}. The scores were converted following the standard scoring procedure: odd numbered items contribute \textit{(response $-1$)}, and even numbered items \textit{(5 $-$ response)}. The final \gls{SUS} score is obtained by multiplying the summed adjusted scores by 2.5, yielding a value in the range 0--100 this equation can be presented as:

\begin{equation}
	\text{SUS} = 2.5 \times 
	\left(
	\sum_{i \in \text{odd}} (R_i - 1)
	\;+\;
	\sum_{j \in \text{even}} (5 - R_j)
	\right)
\end{equation}

\begin{itemize}
	\item $R_i$ = response to item $i$.
	\item Odd items: contribution $(R_i - 1)$.
	\item Even items: contribution $(5 - R_j)$.
	\item Final score $\in [0, 100]$.
\end{itemize}  


Table~\ref{tab:sus_scores} reports the \gls{SUS} scores obtained for each participant.

\bgroup
\rowcolors{1}{}{GhostWhite}
\begin{table}[htbp]
	\centering
	\caption{\gls{SUS} scores obtained per participant}
	\label{tab:sus_scores}
	\begin{tabular}{lcc}
		\toprule
		\rowcolor{Gainsboro}
		Participant & SUS Score (0 - 100) \\
		\midrule
		P1 & 75 \\
		P2 & 67.5 \\
		P3 & 80 \\
		P4 & 85 \\
		P5 & 75 \\
		\bottomrule
	\end{tabular}
\end{table}
\egroup

The aggregated \gls{SUS} scores yielded the following descriptive statistics:

\begin{itemize}
	\item \textbf{Mean \gls{SUS} score:} 76.5
	\item \textbf{Median \gls{SUS} score:} 75.0
	\item \textbf{Standard deviation:} 6.52
\end{itemize}

According to industry benchmarks~\cite{Bangor2009,SauroLewis2016}:

\begin{itemize}
	\item Scores above \textbf{80.3} indicate \emph{excellent} usability (Grade A).
	\item Scores between \textbf{68 and 80.3} indicate \emph{good/acceptable} usability (Grade B).
	\item A score of \textbf{68} represents the conventional acceptability threshold.
	\item Scores below \textbf{50} reflect \emph{poor} usability.
\end{itemize}

Based on these thresholds, the configuration language demonstrated an average usability level of \emph{good/acceptable} ($\bar{x} = 76.5$), indicating that participants perceived the workflow as having a positive usability experience. Having a standard deviation of $\sigma = 6.52$ also puts the average usability on the verge of the acceptability threshold ($\bar{x} - \sigma \approx 69.98$) while the upper level reaches the excellence of usability ($\bar{x} + \sigma \approx 83.02$), indicating that the system is likely to be well received by the majority of the population.

\subsubsection{Observed Patterns}
\label{sec:test_usabl_obs_patterns}

Qualitative comments collected during the SUS administration suggested that:
TODO: delete these if none new
\begin{itemize}
	\item Users with prior \gls{AADL} experience tended to rate the system slightly higher.
	\item The ROS expert (P5) provided a score aligned with the group’s mean, indicating that the \gls{DSL} is understandable even to users arriving from the code-consumer side rather than the modeling side.
	\item Scores correlated positively with low perceived workload in the
	NASA-\gls{TLX} results, reinforcing the observed low cognitive effort.
\end{itemize}



\subsection{Discussion}
\label{sec:test_usabl_obs}

% TODO: Summarize findings and relate them explicitly to usability requirements.
% TODO: Link observations to efficiency, learnability, discoverability, error recovery, etc.
% TODO: Compare results to expectations or prior studies.
% TODO: Identify design issues revealed by qualitative data.

\begin{table}[htbp]
	\centering
	\caption{Qualitative Observations During Testing}
	\label{tab:usability_observations}
	\begin{tabular}{p{3cm} p{10cm}}
		\toprule
		\textbf{Participant} & \textbf{Observations} \\
		\midrule
		P1 & Participant 1 described that the fact that other Eclipse configurations are present on top on the current configuration generates confusion. To minimize this problem, subsequent participants were explicitly told that the only configuration they should change was \textit{CodeGen Config}. \\
		P2 & Participant 2 had a more fluid experience when it came to understanding the code generation sequence given the previous knowledge in \gls{AADL} development. \\
		P3 & Although participant 3 was not familiarized with the Eclipse environment he adapted very quickly and understood the task perfectly, suggesting that the system is intuitive even to newcomers. \\
		P4 &  \\
		P5 &  \\
		\bottomrule
	\end{tabular}
\end{table}

Some participants, particularly 1 and 3, also stated that some task names do not directly map to \gls{UI} options, this was done purposefully in order to test participant adaptability although it revealed that participants expected that the task instruction always mapped to an option, which was not always the case, leading to a few tasks in general taking more than others, for instance \textit{Disable launch file generation} consistently took more time than other tasks given that the launch file is part of the Build Integration (Section~\ref{sec:impl_build_options}) options, however, this name accurately represents the group of options so changing it would undermine the remaining options.

% TODO: Fill this table.
% TODO: Extract themes from qualitative notes.


\subsection{Threats to Validity}
\label{sec:test_usabl_validity}

Although the tests conducted were very promising and revealed a strong structural integrity of the software built, a core weakness of the validation tests was the lack of many participants capable of using \gls{ROS} in a high degree of proficiency. This was mainly due to the difficulty of having contact and time with a \gls{ROS} industry individual (our specific target demographic). Nevertheless, our individual testing proved that at least one \gls{ROS} industry professional approves of the developed configuration language. TODO: verify

Some of the test participants had brief interactions with a very primitive iteration of \gls{DSL} throughout the development process which might have slightly influenced their performance even though the numbers do not seem to favor any individual in particular. The \gls{DSL} UI was never shown in full, only part of the \gls{metamodel} that represents it, hence the diminished threat of this specific case.

It is natural that users learned over the completion of the tasks, which in turn led to finishing tasks being completed faster. To combat this, the middle section of the tasks (Section B to section E, see Appendix~\ref{app:task_book}) were shuffled in different ways for each participant, which lead to more normalized results. Still, Section F was always evaluated with the experience of all prior tasks, which was mandatory since it was the final code generation step.

Another aspect to consider is the Hawthorne effect~\cite{Hawthorne_effect}, in which participants of a test change their behavior when being observed, which was essentially the exact scenario. Still, it was stated verbally multiple times that the evaluation was not on the user itself but on the tool, which might have dimished that effect minimally.

Sampling was a major issue for the tests, mainly since the tool is targeting a specific industry user, which reduces the participant pool significantly, due to that factor, the sample size ended up being smaller than what many experts consider optimal, however the participants represented the closest to a \gls{ROS} expert user. Still, that representation is not 100\% accurate since none of the users was an actual, multi-year working industry \gls{ROS} expert.

The usability tests measure many types of data, however, these tests should not be taken as a complete analysis of usability of the code generation customization in \gls{RAMSES}, many more tests should be taken into account, especially tests using the complete \gls{RAMSES} pipeline and not just the code generator, in order to measure all facets of usability.



% TODO: Add internal validity threats: learning effects, evaluator bias, Hawthorne effect, prior exposure.
% TODO: Add external validity threats: small sample size, convenience sampling, non-representative ROS expertise distribution.
% TODO: Add construct validity threats: metrics measure efficiency but not all facets of usability.
% TODO: Add conclusion validity threats: statistical power insufficient for subgroup comparisons.


\subsection{Conclusion}
\label{sec:test_usabl_conclusion}


Many users noted that the \textit{Generation Hooks} were a part of file management, hence should be included in the \textit{Files} submenu. This change was not only logical but also simple and practical since it reduced menu clutter and made the overall \gls{DSL} more organize. The change was implemented immediately after testing was finished.



% TODO: Add a concise summary of whether usability goals were met.
% TODO: List concrete improvements identified.
% TODO: Add future work: larger sample, ROS-expert recruitment, longitudinal testing, benchmarking against alternative tools.





















