%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter6.tex
%% NOVA thesis document file
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter6.tex}%

\chapter{Software Testing}
\label{cha:test}


\epigraph{ \textit{This chapter documents the software testing done throughout the development of the thesis. It highlights how the specific tests where done, in what context and their results.}}

Software testing was conducted specifically for the configuration of the code generation part of \gls{RAMSES}. This means that the goal was to ensure that the newly developed configuration options did not modify the runtime results of the generated software, hence maintaining the core goal of the input model.

\section{Integration Testing}
\label{sec:test_feature}

This type of testing is recurrent along the development, it serves to ensure that the features implemented are functioning as intended and that the previous version of the software has retained its functionality when new features were added.

Integration testing was performed in two clear stages, matching the incremental development of the configuration language. The first phase, in August, ensured that the addition of configuration features did not change the behavior of the baseline generator. The second phase, in October, focused on the validation of the more advanced architectural and runtime-related configuration options and refinement of the testing methodology to better reflect the intended modularity of \gls{RAMSES}.

\subsection{Integration Testing: August}
\label{sec:test_feature_one}

After the development of the first batch of features, the testing served to ensure that the previous version of the code generator was still working as intended and to validate the integration of the code generator configurator in the pipeline.

The main testing environment for the tests in this sub section was the following:

\begin{itemize} 
	\item Eclipse version 2024-03 (4.31)
	\item OSATE2 version 2.16.0.vfinal
	\item RAMSES2 version Aug 26, 2025 (commit 335579b6e58a188475e7a5e7ed099b9a549bfb25)
	\item ROS 2 Humble Hawksbill
	\item Acceleo version 3.7.15.202402190857
	\item MDELab Workflow version 1.9.1.202402292002
	\item EMF - Eclipse Modeling Framework SDK version 2.37.0.v20240203-1010
	\item Ubuntu version 22.04.5 LTS
\end{itemize}

The regression tests applied at this time were limited since the previous version of the software did not include any code generation customization, only the simple transition from model to code (\gls{M2T}), which, after generating a \gls{ROS} project in both the previous code generator and the at the time current one (with the first batch of features implemented), it was possible to verify that both generators still produced the same practical output, essentially, the code produced by both generators yields the same functionality.

The integration test part was done much more in depth since the goal was to intertwine the different new features and find out if they worked in perfect harmony as expected. The tests present in Table~\ref{tab:int_tests_1} were conducted.

\bgroup
\rowcolors{1}{}{GhostWhite}
\begin{xltabular}{\textwidth}{X X X X X}
	\caption{Feature dependency table}
	\label{tab:int_tests_1}\\
	\toprule
	\rowcolor{Gainsboro}%
	Test Case ID & Feature & Description & Expected Outcome \\
	\midrule
	TC-INT-01 & \Gls{identifier} Management & Check that \gls{identifier} config is applied in code, traceability, and report. & Same naming convention is visible in all outputs. \\
	TC-INT-02 & Comments & Verify comment style and formatting are preserved in all files. & All comments retain exact style and placement. \\
	TC-INT-03 & Traceability & Ensure model elements map to the correct code location and report entry. & JSON and HTML show accurate paths and line numbers. \\
	TC-INT-04 & Report Generation & Ensure generation metrics are passed from Java generator to report. & Report displays correct counts and settings. \\
	TC-INT-05 & Code Quality \& Report Sync & Verify quality checker warnings appear in both XML and report. & Both outputs show the same warning details. \\
	TC-INT-07 & Cross-Feature Stability & Test large models with custom \glspl{identifier}, comments, and style violations. & All features work together without breaking. \\
\end{xltabular}

After conducting these tests, the following was concluded:

\begin{itemize}
	\item \textbf{TC-INT-01 - \Gls{identifier} Consistency:} Verified that the \gls{identifier} configuration set in the \texttt{config.generator} model is applied consistently in generated C++ code and reflected in the traceability file as well as in the report.
	
	\item \textbf{TC-INT-02 - Comments:} Generated code containing both single-line and multi-line comments, verified that:
	\begin{enumerate}
		\item Acceleo templates correctly output the comments.
		\item Java generator preserves formatting.
	\end{enumerate}
	
	\item \textbf{TC-INT-03 - Traceability Link Accuracy:} Confirmed that model elements are mapped to the correct files and line numbers in generated code, and that these mappings are:
	\begin{enumerate}
		\item Present in the \gls{JSON} traceability output.
		\item Referenced in the \gls{HTML} report.
	\end{enumerate}
	
	\item \textbf{TC-INT-04 - Report Generation Data Flow:} Metrics collected during generation (number of components, code quality warnings) are passed to the Acceleo generator rendered correctly in the report.
	
	\item \textbf{TC-INT-05 - Code Quality \& Report Synchronization:} Triggered a known style violation in the model, generated code, and verified that:
	\begin{enumerate}
		\item The code quality checker detects it and outputs XML accurately referencing it.
		\item The Java generator correctly embeds this warning into the final HTML report.
	\end{enumerate}
	
	\item \textbf{TC-INT-07 - Cross-Feature Stability:} Used a large, complex model with:
	\begin{itemize}
		\item Custom \glspl{identifier}
		\item Extensive comments
		\item Known style violations
	\end{itemize}
	Verified that all features (\gls{identifier} config, comments, traceability, reporting, quality checking) work together without breaking any module.
\end{itemize}



\subsection{Integration Testing: October}
\label{sec:test_feature_two}

This latter stage of integration testing concentrated on the more architecture and runtime-oriented features of configuration, as well as revising the way in which testing was performed in order to better align with the modular design principles of \gls{RAMSES}. While tests in August concentrated on baseline correctness and compatibility, tests in October focused on verifying that these advanced configuration options (business logic decoupling, executor selection, file management, and build behavior) interoperate correctly and produce expected behavior when combined.

The testing environment remained mostly the same, with the exception of the execution platform: testing was moved from a physical Ubuntu installation on Windows to a dedicated Linux virtual machine. The \gls{RAMSES} version was also updated to Oct 27, 2025 (commit 3eb5118b57903cc5077ad4218fc83e182cbab199).

The initial tests were conducted with a "parameterized" way in mind, meaning that the architecture prioritized code efficiency albeit with an addition of complexity. This in of itself is not an issue at all, the tests were still very efficient, accurate and highly maintainable\footnote{To add a new test only one function needs to be added, at least.}. However, the current \gls{RAMSES} architecture benefited more from running tests in an isolated, standalone form, meaning that it would be better if it was possible to run specific test of the generated code without the need to run the whole test strip. For this goal, the parameterized way of testing was inefficient, since it entails that every test is generic and there is no way to quickly modify specific parameters in a single test.

So, the test class was re-written to have one method per test, per model. Meaning that instead of ten generic tests ran with parameterized values, there would be fifty \footnote{At that point in time, with five models.} specific tests, ten for each test model.

The features tested in Section~\ref{sec:test_feature_one} were re-tested and their behavior remained, as expected, the same. On top of that, new integration tests were performed to ensure that new features behave as intended, mainly:

\begin{itemize}
	\item \textbf{TC-INT-08 - Business Logic Decoupling:} Verified that when the DSL specifies decoupled business logic, the generated code separates logic into independent modules, allowing tasks to execute without unnecessary dependencies.
	
	\item \textbf{TC-INT-09 - Executor Behavior:} Configured models with different executor types and confirmed that:
	\begin{enumerate}
		\item Multi-threaded execution logic is correctly generated when requested.
		\item Static single-thread execution is applied as configured.
	\end{enumerate}
	
	\item \textbf{TC-INT-10 - File Management:} Generated code using models with various file settings and confirmed that:
	\begin{enumerate}
		\item Existing files remain unchanged when overwriting is disabled.
		\item Configured header prefixes are correctly applied in all files.
	\end{enumerate}
	
	\item \textbf{TC-INT-11 - Extension Script Execution:} Added pre and post generation scripts to the DSL and verified that:
	\begin{enumerate}
		\item Pre-generation scripts execute successfully before code generation starts.
		\item Post-generation scripts run correctly after generation completes.
	\end{enumerate}
	
	\item \textbf{TC-INT-12 - Build Integration Artifacts:} Configured build integration options in the DSL and confirmed that:
	\begin{enumerate}
		\item Source files, Makefile, package files, and launch scripts are generated according to the configuration.
		\item Disabling specific build outputs correctly prevents their generation without affecting other features.
	\end{enumerate}
	
	\item \textbf{TC-INT-13 - Legacy Library Handling:} Added legacy library configuration to the DSL and verified that:
	\begin{enumerate}
		\item Library headers are correctly included in the generated code.
		\item Libraries are copied into the project folder when requested.
	\end{enumerate}
	
	\item \textbf{TC-INT-14 - Target OS Adaptation:} Configured different target operating systems and confirmed that:
	\begin{enumerate}
		\item Generated code includes OS-specific adaptations for Linux, Windows, and MacOS.
	\end{enumerate}
\end{itemize}

These tests ensured that the new features provided, in fact, additional and specific functionalities that work as predicted.


\section{Regression Testing}
\label{sec:test_unit}

In order to automate the testing of the added features as development progressed unit tests were developed. By comparing the functional version of the generated projects against a newly generated version of the same project, we are able to detect differences in the end result which can point out flaws when a branch of the code generation is modified. 

The testing environment remained the same, with only the \gls{RAMSES} version changing throughout the development.

Essentially, what we test can be visually described by Figure~\ref{fig:testingFlowchart}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.55\textwidth]{testingFlowchart.png}
	\caption{Flowchart of the testing strategy for generated code (made by the author)}
	\label{fig:testingFlowchart}
\end{figure}

By using this architecture, it is only necessary to verify the correctness of the generated code once. The subsequent generations can be then compared to this already correct premise to determine errors or changes in the code that are not intended.

This verified control code was tested thoroughly by manually generating, compiling, deploying to a target system and executing a series of runtime tests to confirm its functionality.

An obvious flaw of this model is that, not only does it need complete manual validation in the start, but it's also very rigid when it comes to file comparison, comparing each file line-by-line\footnote{The comparison is not overly rigid and allows for exceptions such as the generated date and order changes in \gls{XML} files.}.

These flaws can also be seen as benefits, since the code needs to be tested rigorously to ensure maximum code quality when generated. This can only be achieve by actually executing the generated projects and verifying their functionality manually.

The unit tests were made to be as flexible as possible, allowing for different configurations and different models to be tested in a single run, providing insights in multiple generated projects in a single test execution as can be observed in Figure~\ref{fig:testingResults1}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.55\textwidth]{testingResults1.png}
	\caption{Testing results of 3 different models}
	\label{fig:testingResults1}
\end{figure}

The tests also account for certain projects not generating unneeded files such as the traceability or the report file. This behavior is translated by simply skipping the test as can be observed in Figure~\ref{fig:testingResultsSkip1}. Similar tests also verify the existence of folders and files that are a requirement for any \gls{ROS} project and need to be present after generation.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.55\textwidth]{testingResultsSkip1.png}
	\caption{Skipped Trace, CppCheck and Report files as those are not present}
	\label{fig:testingResultsSkip1}
\end{figure}

In addition, the test runs also validate the existence of executable both in the launch and make configurations, notably these executables have to exists in both files. 

As a guardrail, if files are present in the control project but not in the generated project that will generate an error and vice versa. This ensures that the generator produces only what is needed and no file is missing.

\bgroup
\rowcolors{1}{}{GhostWhite}
\begin{xltabular}{\textwidth}{X X X X X}
\caption{Regression test cases and expected outcomes}
\label{tab:reg_tests_1}\\
\toprule
\rowcolor{Gainsboro}%
Test Case ID & Feature & Description & Expected Outcome \\
\midrule
TC-REG-01 & Regression & Compare output with older version for unchanged models. & Output is functionally identical. \\
TC-REG-02 & Regression & Generate code for previously passing models. & Generation succeeds with no new errors. \\
TC-REG-03 & Regression & Measure generation time for unchanged models. & Timing does not exceed threshold. \\
TC-REG-04 & Regression & Test legacy models without new features enabled. & Output has no formatting or syntax changes. \\
\end{xltabular}


The tests described in table~\ref{tab:reg_tests_1} were proven true via \gls{JUnit} tests, which not only were able to verify the present tests but also additional ones that are not present in this table. The full detailed list of tests can be found in the Appendix~\ref{app:unit_tests}.

TC-REG-02, TC-REG-03 and TC-REG-04 were easily proven by the unit test strip since this process can be automated and viewed quickly. TC-REG-01 added a layer of complexity because it pushed for manual verification which had to be done to ensure the functionality of the output code remained the same as the control group.

All the unit tests previously described in this section (Section~\ref{sec:test_unit}) were initially done as plug-in tests, however, due to a multitude of reasons, they were later changed to standalone tests.

\begin{itemize}  
	\item \textbf{Plug-in unit tests} are tests done by launching a new Eclipse instance that loads all the resources necessary and executes the unit tests. This approach is simpler to setup but takes significantly more time to run than the standalone option due to the creation of the additional Eclipse instance.
	\item \textbf{Standalone unit tests} do not launch a new Eclipse instance, instead they run detached from eclipse, which makes them faster with the only downside being they require more initial setup to initialize the resources and dependencies that Eclipse would normally provide.
\end{itemize}

The change from plug-in tests to standalone revealed a clear advantage when it came to test execution time, out of 20 executions there was an average of 20\% of time saved, which can be attributed to the standalone runs not having to initialize a new instance of Eclipse. This time reduction is inversely proportional to the amount of tests, as in, less tests will translate to an even faster average of time saved by the standalone execution, which is especially useful when running single or low amounts of tests, having an average of 60\% of time saved. Even when running large amounts of tests (>100) there is still a reduced execution time on the standalone run, albeit not as noticeable as before with around 10\% reduction. The test run times can be seem in the Appendix~\ref{app:unit_test_times}.



\section{Usability Validation}
\label{sec:test_val}

With the \gls{DSL} finalized and its features implemented it is then possible to asses how an end user will interact with the newly developed features.

For this test, 5 users with a moderate/high familiarization to \gls{AADL} were picked to perform a sequence of tasks that mimicked the usage of the \gls{DSL} to its near fullest extent.

The goal was not only to acquire real data in a real environment but also to highlight any flaws or issues that might have been overlooked in the development phase and to verify whether the configuration language meets the usability requirements defined earlier in Chapter~\ref{chap:challenges_and_requirements}, particularly its accessibility to users with different levels of technical expertise.

\subsection{Participants}
\label{sec:test_usabl_participants}
TODO: nr participants participants were selected for this study. All were familiar with \gls{AADL} and had experience with model-driven workflows, even though their backgrounds and technical depth varied. This group reflects the typical user base within the RAMSES ecosystem: developers, testers, and stakeholders who understand AADL semantics but may not necessarily be expert programmers. Participants remained anonymous.

\subsection{Tasks}
\label{sec:test_usabl_tasks}
Each participant was asked to perform a sequence of tasks designed to cover every major feature of the configuration language. These tasks simulate realistic use of the \gls{DSL} and its integration with \gls{RAMSES}. The task set was organized into five groups:

\begin{itemize}
	\item \textbf{Basic Configuration:} selecting the target language, target operating system, and output folder.
	\item \textbf{Identifier and Naming Options:} changing identifier style, applying prefixes or suffixes, and switching to the \textit{STANDARD} naming convention.
	\item \textbf{Feature Toggles:} enabling or disabling comments, report generation, traceability, and file management options.
	\item \textbf{Advanced Capabilities:} configuring executor type, managing business logic decoupling, referencing legacy libraries, and defining pre and post generation hooks.
	\item \textbf{Full Workflow Validation:} triggering code generation and verifying that the outputs (generated code, reports, trace files, etc) reflect the selected configuration.
\end{itemize}

The task sequence was intentionally designed to exercise the \gls{DSL} to near full extent without requiring prior explanation of its internal implementation.

\subsection{Evaluation Criteria}
\label{sec:test_usabl_eval_crit}
The usability evaluation followed the methodology defined in the requirements phase of the project. Both quantitative and qualitative measures were collected to assess the usability of the configuration interface and the effort required to use it.

\paragraph{Quantitative data}
\label{sec:test_usabl_quant}
For every task and participant, the following metrics were recorded:

\begin{itemize}
	\item Task completion time.
	\item Number and type of errors committed.
	\item Number of help requests or clarifications needed.
\end{itemize}

\paragraph{Qualitative data}
\label{sec:test_usabl_qual}
During the session, the following observations were captured:

\begin{itemize}
	\item Visible misunderstandings and their severity.
	\item User comments and verbalized expectations.
	\item Indicators of the learning curve across tasks (increased speed or confidence).
\end{itemize}

\paragraph{Subjective evaluation}
\label{sec:test_usabl_subj}
After completing all tasks, each participant completed two standardized questionnaires:

\begin{itemize}
	\item \textbf{\gls{SUS}} for perceived usability.
	\item \textbf{NASA \gls{TLX}} to assess perceived workload and cognitive demands.
\end{itemize}

These instruments provide widely accepted measures for usability and mental effort. They also enable a structured and comparable assessment.

\subsection{Execution}
\label{sec:test_usabl_proc}
Each session followed the same procedure:

\begin{enumerate}
	\item \textbf{Introduction:} participants were informed that they would configure a code generator using the provided interface, without receiving task-specific guidance.
	\item \textbf{Task execution:} participants completed the predefined task sequence while the evaluator recorded times, errors, and observations. No assistance was offered unless explicitly requested.
	\item \textbf{Post-study questionnaires:} participants completed the \gls{SUS} and NASA-\gls{TLX} questionnaires.
	\item \textbf{Final debriefing:} participants were invited to provide additional comments and suggestions.
\end{enumerate}

All sessions were conducted individually and under identical software environments to ensure comparability.

\subsection{Analysis}
\label{sec:test_usabl_analysis}
With all data collected, we can proceed with the analysis.

\begin{table}[htbp]
	\centering
	\caption{Usability Task Performance Summary}
	\label{tab:usability_tasks}
	\begin{tabular}{lccc}
		\toprule
		\textbf{Task} & \textbf{Completion Time (s)} & \textbf{Errors} & \textbf{Help Requests} \\
		\midrule
		Select target language &  &  &  \\
		Configure naming style &  &  &  \\
		Apply identifier prefix/suffix &  &  &  \\
		Enable/disable comments &  &  &  \\
		Enable/disable report generation &  &  &  \\
		Enable/disable traceability &  &  &  \\
		Change executor type &  &  &  \\
		Toggle business logic decoupling &  &  &  \\
		Configure legacy libraries &  &  &  \\
		Add pre/post scripts &  &  &  \\
		Trigger code generation &  &  &  \\
		Verify outputs &  &  &  \\
		\midrule
		\textbf{Totals} &  &  &  \\
		\bottomrule
	\end{tabular}
\end{table}

Table~\ref{tab:usability_tasks} illustrates the aggregated performance summary of the participants. The completion time is very much acceptable for most tasks given the lack of familiarity with the software that every participant had. We can see a spike when it comes to \gls{ROS} specific things that the casual \gls{AADL} user might not be familiar with, for instance, changing the thread executor, managing the business logic, etc.

The amount of error performed was below expectations, most likely due to the simplicity and structure of the configuration language.

Nevertheless, this errors where quantified in the following Table~\ref{tab:usability_errors}

\begin{table}[htbp]
	\centering
	\caption{Categorization of User Errors}
	\label{tab:usability_errors}
	\begin{tabular}{lcl}
		\toprule
		\textbf{Error Category} & \textbf{Count} & \textbf{Description} \\
		\midrule
		Navigation error &  & Incorrectly locating the configuration page or submenu. \\
		Misinterpretation &  & Misunderstanding the meaning of an option. \\
		Configuration misuse &  & Selecting an option that conflicts with the intended task. \\
		UI confusion &  & Confusion caused by labels, layouts, or grouping. \\
		Missing confirmation &  & Forgetting to apply/save configuration changes. \\
		Other &  &  \\
		\bottomrule
	\end{tabular}
\end{table}


\section*{NASA \gls{TLX}}

\textbf{Dimensions:}
\begin{itemize}
	\item Mental Demand
	\item Physical Demand
	\item Temporal Demand
	\item Performance
	\item Effort
	\item Frustration
\end{itemize}

Each item is scored from 0 to 100.


\begin{table}[htbp]
	\centering
	\caption{NASA-\gls{TLX} Scores per Participant}
	\label{tab:nasatlx}
	\begin{tabular}{lcccccc}
		\toprule
		Participant & Mental & Physical & Temporal & Performance & Effort & Frustration \\
		\midrule
		P1 &  &  &  &  &  &  \\
		P2 &  &  &  &  &  &  \\
		P3 &  &  &  &  &  &  \\
		P4 &  &  &  &  &  &  \\
		P5 &  &  &  &  &  &  \\
		\bottomrule
	\end{tabular}
\end{table}



\subsection{Discussion}
\label{sec:test_usabl_obs}


\begin{table}[htbp]
	\centering
	\caption{Qualitative Observations During Testing}
	\label{tab:usability_observations}
	\begin{tabular}{p{3cm} p{10cm}}
		\toprule
		\textbf{Participant} & \textbf{Observations} \\
		\midrule
		P1 &  \\
		P2 &  \\
		P3 &  \\
		P4 &  \\
		P5 &  \\
		\bottomrule
	\end{tabular}
\end{table}



\subsection{Threats to Validity}
\label{sec:test_usabl_validity}

Although the tests conducted were very promising and revealed a strong structural integrity of the software built, a core weakness of the validation tests was the lack of many participants capable of using \gls{ROS} in a high degree of proficiency. This was mainly due to the difficulty of having contact and time with a \gls{ROS} industry individual (our specific target demografic). Nevertheless, our individual testing proved that at least one \gls{ROS} industry professional approves of the developed configuration language. TODO: verify

Some of the test participants had brief interactions with the \gls{DSL} throughout the development process which might have slightly influenced their performance even though the numbers do not seem to favor any individual in particular. TODO: verify



















