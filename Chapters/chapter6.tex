%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter6.tex
%% NOVA thesis document file
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter6.tex}%

\chapter{Software Testing}
\label{cha:test}


\epigraph{ \textit{This chapter documents the software testing done throughout the development of the thesis. It highlights how the specific tests where done, in what context and their results.}}

Software testing was conducted specifically for the configuration of the code generation part of \gls{RAMSES}. This means that the goal was to ensure that the newly developed configuration options did not modify the runtime results of the generated software, hence maintaining the core goal of the input model.

\section{Feature Testing}
\label{sec:test_feature}

This type of testing is recurrent along the development, it serves to ensure that the features implemented are functioning as intended and that the previous version of the software has retained its functionality when new features were added.

\subsection{Feature Testing: August}
\label{sec:test_feature_one}

After the development of the first batch of features, the testing served to ensure that the previous version of the code generator was still working as intended and to validate the integration of the code generator configurator in the pipeline.

The report for the first volley of tests can be found in the Annex~\ref{label}, where it is explained the purpose of each individual test and the results they withheld.

The regression tests applied at this time were limited since the previous version of the software did not include any code generation customization, only the simple transition from model to code (\gls{M2T}), which, after generating a \gls{ROS} project in both the previous code generator and the at the time current one (with the first batch of features implemented), it was possible to verify that both generators still produced the same practical output, essentially, the code produced by both generators yields the same functionality.

The integration test part was done much more in depth since the goal was to intertwine the different new features and find out if they worked in perfect harmony as expected. The tests present in Table~\ref{tab:int_tests_1} were conducted.

\bgroup
\rowcolors{1}{}{GhostWhite}
\begin{xltabular}{\textwidth}{X X X X X}
	\caption{Feature dependency table}
	\label{tab:int_tests_1}\\
	\toprule
	\rowcolor{Gainsboro}%
	Test Case ID & Feature & Description & Expected Outcome \\
	\midrule
	TC-INT-01 & \Gls{identifier} Management & Check that \gls{identifier} config is applied in code, traceability, and report. & Same naming convention is visible in all outputs. \\
	TC-INT-02 & Comments & Verify comment style and formatting are preserved in all files. & All comments retain exact style and placement. \\
	TC-INT-03 & Traceability & Ensure model elements map to the correct code location and report entry. & JSON and HTML show accurate paths and line numbers. \\
	TC-INT-04 & Report Generation & Ensure generation metrics are passed from Java generator to report. & Report displays correct counts and settings. \\
	TC-INT-05 & Code Quality \& Report Sync & Verify quality checker warnings appear in both XML and report. & Both outputs show the same warning details. \\
	TC-INT-07 & Cross-Feature Stability & Test large models with custom \glspl{identifier}, comments, and style violations. & All features work together without breaking. \\
\end{xltabular}

After conducting these tests, the following was concluded:

\begin{itemize}
	\item \textbf{TC-INT-01 – \Gls{identifier} Consistency:} Verified that the \gls{identifier} configuration set in the \texttt{config.generator} model is applied consistently in generated C++ code and reflected in the traceability file as well as in the report.
	
	\item \textbf{TC-INT-02 – Comments:} Generated code containing both single-line and multi-line comments, verified that:
	\begin{enumerate}
		\item Acceleo templates correctly output the comments.
		\item Java generator preserves formatting.
	\end{enumerate}
	
	\item \textbf{TC-INT-03 – Traceability Link Accuracy:} Confirmed that model elements are mapped to the correct files and line numbers in generated code, and that these mappings are:
	\begin{enumerate}
		\item Present in the \gls{JSON} traceability output.
		\item Referenced in the \gls{HTML} report.
	\end{enumerate}
	
	\item \textbf{TC-INT-04 – Report Generation Data Flow:} Metrics collected during generation (number of components, code quality warnings) are passed to the Acceleo generator rendered correctly in the report.
	
	\item \textbf{TC-INT-05 – Code Quality \& Report Synchronization:} Triggered a known style violation in the model, generated code, and verified that:
	\begin{enumerate}
		\item The code quality checker detects it and outputs XML accurately referencing it.
		\item The Java generator correctly embeds this warning into the final HTML report.
	\end{enumerate}
	
	\item \textbf{TC-INT-07 – Cross-Feature Stability:} Used a large, complex model with:
	\begin{itemize}
		\item Custom \glspl{identifier}
		\item Extensive comments
		\item Known style violations
	\end{itemize}
	Verified that all features (\gls{identifier} config, comments, traceability, reporting, quality checking) work together without breaking any module.
\end{itemize}



\subsection{Feature Testing: October}
\label{sec:test_feature_two}

Testings in October were done with the intent of solidifying the already robust configuration language, but also to fundamentally change the way tests are performed in order to better adhere to the \gls{RAMSES} architecture logic.

The initial tests were conducted with a "parameterized" way in mind, meaning that the architecture prioritized code efficiency albeit with an addition of complexity. This in of itself is not an issue at all, the tests were still very efficient, accurate and highly maintainable\footnote{To add a new test only one function needs to be added, at least.}. However, the current \gls{RAMSES} architecture benefited more from running tests in an isolated, standalone form, meaning that it would be better if it was possible to run specific test of the generated code without the need to run the whole test strip. For this goal, the parameterized way of testing was inefficient, since it entails that every test is generic and there is no way to quickly modify specific parameters in a single test.

So, the test class was re-written to have one method per test, per model. Meaning that instead of ten generic tests ran with parameterized values, there would be fifty \footnote{At that point in time, with five models.} specific tests, ten for each test model.

The features tested in Section~\ref{sec:test_feature_one} were re-tested and their behavior remained, as expected, the same. On top of that, new integration tests were performed to ensure that new features behave as intended, mainly:

\begin{itemize}
	\item \textbf{TC-INT-08 – Business Logic Decoupling:} Verified that when the DSL specifies decoupled business logic, the generated code separates logic into independent modules, allowing tasks to execute without unnecessary dependencies.
	
	\item \textbf{TC-INT-09 – Executor Behavior:} Configured models with different executor types and confirmed that:
	\begin{enumerate}
		\item Multi-threaded execution logic is correctly generated when requested.
		\item Static single-thread execution is applied as configured.
	\end{enumerate}
	
	\item \textbf{TC-INT-10 – File Management:} Generated code using models with various file settings and confirmed that:
	\begin{enumerate}
		\item Existing files remain unchanged when overwriting is disabled.
		\item Configured header prefixes are correctly applied in all files.
	\end{enumerate}
	
	\item \textbf{TC-INT-11 – Extension Script Execution:} Added pre and post generation scripts to the DSL and verified that:
	\begin{enumerate}
		\item Pre-generation scripts execute successfully before code generation starts.
		\item Post-generation scripts run correctly after generation completes.
	\end{enumerate}
	
	\item \textbf{TC-INT-12 – Build Integration Artifacts:} Configured build integration options in the DSL and confirmed that:
	\begin{enumerate}
		\item Source files, Makefile, package files, and launch scripts are generated according to the configuration.
		\item Disabling specific build outputs correctly prevents their generation without affecting other features.
	\end{enumerate}
	
	\item \textbf{TC-INT-13 – Legacy Library Handling:} Added legacy library configuration to the DSL and verified that:
	\begin{enumerate}
		\item Library headers are correctly included in the generated code.
		\item Libraries are copied into the project folder when requested.
	\end{enumerate}
	
	\item \textbf{TC-INT-14 – Target OS Adaptation:} Configured different target operating systems and confirmed that:
	\begin{enumerate}
		\item Generated code includes OS-specific adaptations for Linux, Windows, and MacOS.
	\end{enumerate}
\end{itemize}

These tests ensured that the new features provided, in fact, additional and specific functionalities that work as predicted.


\section{Unit Testing}
\label{sec:test_unit}

In order to automate the testing of the added features as development progressed unit tests were developed. By comparing the functional version of the generated projects against a newly generated version of the same project, we are able to detect differences in the end result which can point out flaws when a branch of the code generation is modified. 

Essentially, what we test can be visually described by Figure~\ref{fig:testingFlowchart}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.55\textwidth]{testingFlowchart.png}
	\caption{Flowchart of the testing strategy for generated code (made by the author)}
	\label{fig:testingFlowchart}
\end{figure}

By using this architecture, it is only necessary to verify the correctness of the generated code once. The subsequent generations can be then compared to this already correct premise to determine errors or changes in the code that are not intended.

An obvious flaw of this model is that, not only does it need complete manual validation in the start, but it's also very rigid when it comes to file comparison, comparing each file line-by-line\footnote{The comparison is not overly rigid and allows for exceptions such as the generated date and order changes in \gls{XML} files.}.

These flaws can also be seen as benefits, since the code needs to be testes rigorously to ensure maximum code quality when generated. This can only be achieve by actually executing the generated projects and verifying their functionality.

The unit tests were made to be as flexible as possible, allowing for different configurations and different models to be tested in a single run, providing insights in multiple generated projects in a single test execution as can be observed in Figure~\ref{fig:testingResults1}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.55\textwidth]{testingResults1.png}
	\caption{Testing results of 3 different models}
	\label{fig:testingResults1}
\end{figure}

The tests also account for certain projects not generating certain files such as the traceability or the report file. This behavior is translated by simply skipping the test as can be observed in Figure~\ref{fig:testingResultsSkip1}. Similar tests also verify the existence of folders and files that are a requirement for any \gls{ROS} project and need to be present after generation.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.55\textwidth]{testingResultsSkip1.png}
	\caption{Skipped Trace, CppCheck and Report files as those are not present}
	\label{fig:testingResultsSkip1}
\end{figure}

In addition, the test runs also validate the existence of executable both in the launch and make configurations, notably these executables have to exists in both files. 

As a guardrail, if files are present in the control project but not in the generated project that will generate an error and vice versa. This ensures that the generator produces only what is needed and no file is missing.

\bgroup
\rowcolors{1}{}{GhostWhite}
\begin{xltabular}{\textwidth}{X X X X X}
\caption{Regression test table}
\label{tab:reg_tests_1}\\
\toprule
\rowcolor{Gainsboro}%
Test Case ID & Feature & Description & Expected Outcome \\
\midrule
TC-REG-01 & Regression & Compare output with older version for unchanged models. & Output is functionally identical. \\
TC-REG-02 & Regression & Generate code for previously passing models. & Generation succeeds with no new errors. \\
TC-REG-03 & Regression & Measure generation time for unchanged models. & Timing does not exceed threshold. \\
TC-REG-04 & Regression & Test legacy models without new features enabled. & Output has no formatting or syntax changes. \\
\end{xltabular}


The tests described in table~\ref{tab:reg_tests_1} were proven true via \gls{JUnit} tests, which not only were able to verify the present tests but also additional ones that are not present in this table. The full detailed list of tests can be found in the Appendix~\ref{app:unit_tests}















